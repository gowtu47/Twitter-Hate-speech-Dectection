{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight as cw\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Deep Learning Model - Keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Deep Learning Model - Keras - RNN\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, SpatialDropout1D, SimpleRNN, Bidirectional, MaxPooling1D, Conv1D\n",
    "\n",
    "# Deep Learning Model - Keras - General\n",
    "from keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU, PReLU, Lambda, Multiply\n",
    "\n",
    "# Deep Learning Parameters - Keras\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'new_cleaned_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 184354 entries, 0 to 184353\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   is_offensive  184354 non-null  int64 \n",
      " 1   new_text      184274 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_offensive</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go village pump suggest change language rfc set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>anti greek nationalis wikipedia hi alexikoua y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>dis hoe wasnt dis violent lottery ticket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>better atabay helping banned vandals pushing pov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>camelcase sicko camelcase camelcase rule r bal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_offensive                                           new_text\n",
       "0             0    go village pump suggest change language rfc set\n",
       "1             1  anti greek nationalis wikipedia hi alexikoua y...\n",
       "2             1           dis hoe wasnt dis violent lottery ticket\n",
       "3             0   better atabay helping banned vandals pushing pov\n",
       "4             0  camelcase sicko camelcase camelcase rule r bal..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_offensive</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184349</th>\n",
       "      <td>0</td>\n",
       "      <td>template uw vandalism talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184350</th>\n",
       "      <td>1</td>\n",
       "      <td>regrets pussies shit happens deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184351</th>\n",
       "      <td>0</td>\n",
       "      <td>could possibly origin popular game series halo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184352</th>\n",
       "      <td>0</td>\n",
       "      <td>article submission declined wikipedia talk art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184353</th>\n",
       "      <td>0</td>\n",
       "      <td>editors move articles except inside user space...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_offensive                                           new_text\n",
       "184349             0                         template uw vandalism talk\n",
       "184350             1                  regrets pussies shit happens deal\n",
       "184351             0     could possibly origin popular game series halo\n",
       "184352             0  article submission declined wikipedia talk art...\n",
       "184353             0  editors move articles except inside user space..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['new_text'].astype(str)\n",
    "y = df['is_offensive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', min_df=0.0001)\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "texts_train, texts_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(class_weight='balanced', dual=False, max_iter=100000.0, tol=0.01)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearSVC(class_weight=\"balanced\", dual=False, tol=1e-2, max_iter=1e5)\n",
    "model.fit(texts_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(texts_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42732  1560]\n",
      " [ 1175  9840]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9505487551304537"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.97      0.96      0.97     44292\n",
      "     class 1       0.86      0.89      0.88     11015\n",
      "\n",
      "    accuracy                           0.95     55307\n",
      "   macro avg       0.92      0.93      0.92     55307\n",
      "weighted avg       0.95      0.95      0.95     55307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets=['class 0', 'class 1']\n",
    "print(classification_report(y_test, pred, target_names=targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "  \n",
    "  \n",
    "# Save the model as a pickle in a file\n",
    "joblib.dump(model, 'svm.pkl')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model1=MultinomialNB()\n",
    "model1.fit(texts_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=model1.predict(texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41700  2592]\n",
      " [ 1254  9761]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9304608819860054"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, recall and F1-Score for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.97      0.94      0.96     44292\n",
      "     class 1       0.79      0.89      0.84     11015\n",
      "\n",
      "    accuracy                           0.93     55307\n",
      "   macro avg       0.88      0.91      0.90     55307\n",
      "weighted avg       0.93      0.93      0.93     55307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets=['class 0', 'class 1']\n",
    "print(classification_report(y_test, pred1, target_names=targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of K Nearest Neighbours Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model2=KNeighborsClassifier(n_neighbors=9, algorithm='auto')\n",
    "model2.fit(texts_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2=model2.predict(texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43774   518]\n",
      " [ 3898  7117]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92015477245195"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.99      0.95     44292\n",
      "     class 1       0.93      0.65      0.76     11015\n",
      "\n",
      "    accuracy                           0.92     55307\n",
      "   macro avg       0.93      0.82      0.86     55307\n",
      "weighted avg       0.92      0.92      0.91     55307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets=['class 0', 'class 1']\n",
    "print(classification_report(y_test, pred2, target_names=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "model3=AdaBoostClassifier(n_estimators=200)\n",
    "model3.fit(texts_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3=model3.predict(texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix of Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43894   398]\n",
      " [ 2539  8476]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9468964145587357"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.99      0.97     44292\n",
      "     class 1       0.96      0.77      0.85     11015\n",
      "\n",
      "    accuracy                           0.95     55307\n",
      "   macro avg       0.95      0.88      0.91     55307\n",
      "weighted avg       0.95      0.95      0.94     55307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets=['class 0', 'class 1']\n",
    "print(classification_report(y_test, pred3, target_names=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model4=RandomForestClassifier(n_estimators=200)\n",
    "model4.fit(texts_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4=model4.predict(texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43592   700]\n",
      " [ 1934  9081]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523749254163126"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.96      0.98      0.97     44292\n",
      "     class 1       0.93      0.82      0.87     11015\n",
      "\n",
      "    accuracy                           0.95     55307\n",
      "   macro avg       0.94      0.90      0.92     55307\n",
      "weighted avg       0.95      0.95      0.95     55307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets=['class 0', 'class 1']\n",
    "print(classification_report(y_test, pred4, target_names=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2 = df['new_text'].astype(str)\n",
    "y2 = df['is_offensive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 0 0]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(y2)\n",
    "print(Y)\n",
    "Y = to_categorical(Y)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(texts2, Y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82808     clarify identities major sides controversy rel...\n",
       "54663     think terribly big deal think x seems pretty m...\n",
       "95604     suntem forta way look might discover something...\n",
       "128637    niggas like see side bitch cashin another nigg...\n",
       "130697    thanks attention filled information link provi...\n",
       "                                ...                        \n",
       "40885     apparently manchester trafford boundaries foll...\n",
       "43024     yes course edit romanian footballer past prese...\n",
       "75751     cameron ward dj deleted page intrest people no...\n",
       "157141    probably know understand new warned npa actual...\n",
       "93424     substantive proposal agree significant coverag...\n",
       "Name: new_text, Length: 156700, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 7619, 1167, 2162],\n",
       "       [   0,    0,    0, ...,  739,  209,  622],\n",
       "       [   0,    0,    0, ..., 8810,    4,  347],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  234, 7245,  967],\n",
       "       [   0,    0,    0, ...,   23,  114, 1425],\n",
       "       [   0,    0,    0, ...,    1,  120,  345]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(LSTM(196, dropout = 0.3, recurrent_dropout = 0.3 ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 128)          1280000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 150, 128)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 196)               254800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 196)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               19700     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,554,702\n",
      "Trainable params: 1,554,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "learning_rate = 0.001\n",
    "# optimizer = Adam(learning_rate)\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "optimizer = Adadelta(learning_rate=1.0, rho=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', \n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000140E501E0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000140E501E0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1102/1102 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9414WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000140E7599E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000140E7599E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1102/1102 [==============================] - 2034s 2s/step - loss: 0.1641 - accuracy: 0.9414 - val_loss: 0.1296 - val_accuracy: 0.9570\n",
      "Epoch 2/5\n",
      "1102/1102 [==============================] - 1951s 2s/step - loss: 0.1098 - accuracy: 0.9611 - val_loss: 0.1137 - val_accuracy: 0.9588\n",
      "Epoch 3/5\n",
      "1102/1102 [==============================] - 1905s 2s/step - loss: 0.0991 - accuracy: 0.9637 - val_loss: 0.1177 - val_accuracy: 0.9599\n",
      "Epoch 4/5\n",
      "1102/1102 [==============================] - 1905s 2s/step - loss: 0.0933 - accuracy: 0.9654 - val_loss: 0.1179 - val_accuracy: 0.9599\n",
      "Epoch 5/5\n",
      "1102/1102 [==============================] - 1906s 2s/step - loss: 0.0853 - accuracy: 0.9683 - val_loss: 0.1189 - val_accuracy: 0.9595\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "X_train_seq,\n",
    "Y_train,\n",
    "batch_size=batch_size,\n",
    "epochs=epochs,\n",
    "verbose=verbose,\n",
    "validation_split=validation_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000140E61C6D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000140E61C6D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict(X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(X_test_seq),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_arg=np.argmax(Y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9604397193896"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy_score(y_test_arg, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     22114\n",
      "           1       0.94      0.86      0.90      5540\n",
      "\n",
      "    accuracy                           0.96     27654\n",
      "   macro avg       0.95      0.92      0.94     27654\n",
      "weighted avg       0.96      0.96      0.96     27654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets=['0', '1']\n",
    "print(classification_report(y_test_arg, pred, target_names=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.16410532593727112,\n",
       "  0.10976911336183548,\n",
       "  0.0991462990641594,\n",
       "  0.09328145533800125,\n",
       "  0.08528274297714233],\n",
       " 'accuracy': [0.9413599967956543,\n",
       "  0.961128830909729,\n",
       "  0.963709831237793,\n",
       "  0.9654399752616882,\n",
       "  0.9682620763778687],\n",
       " 'val_loss': [0.12957754731178284,\n",
       "  0.11374588310718536,\n",
       "  0.11774929612874985,\n",
       "  0.117925263941288,\n",
       "  0.11885814368724823],\n",
       " 'val_accuracy': [0.9569878578186035,\n",
       "  0.958774745464325,\n",
       "  0.9599234461784363,\n",
       "  0.9598596096038818,\n",
       "  0.9594767093658447]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  414,  732,    3,   81, 3483,    3,   43,\n",
       "         470,   26,   18,   14,  470,   26,    3, 2395, 4139, 2583, 4004,\n",
       "         185,   26, 2406, 3483,    3, 7355, 1046]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=['anti greek nationalis wikipedia hi alexikoua yous vandalise wikipedia editing false information may chauvinist edit false information wikipedia nowhere percent greeks albania real information census vandalise wikipedia militant ideas']\n",
    "test_X_seq = tokenizer.texts_to_sequences(v)\n",
    "test_X_seq = pad_sequences(test_X_seq, maxlen=150)\n",
    "test_X_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypreds = np.argmax(model.predict(test_X_seq),axis=1)\n",
    "ypreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL\n"
     ]
    }
   ],
   "source": [
    "if ypreds==1:\n",
    "    print(\"hate\")\n",
    "else:\n",
    "    print(\"NORMAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 150, 128)          1280000   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 150, 128)          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 149, 64)           16448     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 74, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 73, 64)            8256      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 36, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 35, 32)            4128      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 17, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 16, 32)            2080      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,311,746\n",
      "Trainable params: 31,746\n",
      "Non-trainable params: 1,280,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f_model = Sequential()\n",
    "f_model.add(Embedding(max_words, 128, input_length=max_len,\n",
    "                    trainable=False))\n",
    "\n",
    "f_model.add(Dropout(0.2))\n",
    "\n",
    "f_model.add(Conv1D(64,2,padding='valid',activation='relu'))\n",
    "f_model.add(MaxPooling1D())\n",
    "f_model.add(Conv1D(64,2,padding='valid',activation='relu'))\n",
    "f_model.add(MaxPooling1D())\n",
    "\n",
    "f_model.add(Conv1D(32,2,padding='valid',activation='relu'))\n",
    "f_model.add(MaxPooling1D())\n",
    "f_model.add(Conv1D(32,2,padding='valid',activation='relu'))\n",
    "f_model.add(GlobalMaxPooling1D())\n",
    "\n",
    "f_model.add(Dense(16, activation='relu'))\n",
    "f_model.add(Dense(16, activation='relu'))\n",
    "f_model.add(Dropout(0.2))\n",
    "\n",
    "f_model.add(Dense(2, activation='softmax'))\n",
    "f_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000140EA066048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000140EA066048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1959/1959 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.8060WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000140E61D4AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000140E61D4AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1959/1959 [==============================] - 82s 41ms/step - loss: 0.4293 - accuracy: 0.8060 - val_loss: 0.3908 - val_accuracy: 0.8256\n",
      "Epoch 2/10\n",
      "1959/1959 [==============================] - 85s 43ms/step - loss: 0.3761 - accuracy: 0.8258 - val_loss: 0.3660 - val_accuracy: 0.8305\n",
      "Epoch 3/10\n",
      "1959/1959 [==============================] - 76s 39ms/step - loss: 0.3604 - accuracy: 0.8313 - val_loss: 0.3628 - val_accuracy: 0.8327\n",
      "Epoch 4/10\n",
      "1959/1959 [==============================] - 76s 39ms/step - loss: 0.3533 - accuracy: 0.8340 - val_loss: 0.3601 - val_accuracy: 0.8333\n",
      "Epoch 5/10\n",
      "1959/1959 [==============================] - 77s 39ms/step - loss: 0.3482 - accuracy: 0.8354 - val_loss: 0.3639 - val_accuracy: 0.8307\n",
      "Epoch 6/10\n",
      "1959/1959 [==============================] - 77s 39ms/step - loss: 0.3442 - accuracy: 0.8368 - val_loss: 0.3516 - val_accuracy: 0.8347\n",
      "Epoch 7/10\n",
      "1959/1959 [==============================] - 76s 39ms/step - loss: 0.3407 - accuracy: 0.8375 - val_loss: 0.3548 - val_accuracy: 0.8334\n",
      "Epoch 8/10\n",
      "1959/1959 [==============================] - 81s 41ms/step - loss: 0.3384 - accuracy: 0.8382 - val_loss: 0.3542 - val_accuracy: 0.8347\n",
      "Epoch 9/10\n",
      "1959/1959 [==============================] - 84s 43ms/step - loss: 0.3352 - accuracy: 0.8394 - val_loss: 0.3496 - val_accuracy: 0.8347\n",
      "Epoch 10/10\n",
      "1959/1959 [==============================] - 80s 41ms/step - loss: 0.3339 - accuracy: 0.8395 - val_loss: 0.3511 - val_accuracy: 0.8354\n"
     ]
    }
   ],
   "source": [
    "history2 = f_model.fit(X_train_seq, Y_train, batch_size= 64, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4292601943016052,\n",
       "  0.37614789605140686,\n",
       "  0.3603665828704834,\n",
       "  0.3533201217651367,\n",
       "  0.3482048213481903,\n",
       "  0.34419628977775574,\n",
       "  0.34072455763816833,\n",
       "  0.33839714527130127,\n",
       "  0.3352087736129761,\n",
       "  0.3338567912578583],\n",
       " 'accuracy': [0.8060466051101685,\n",
       "  0.8257578015327454,\n",
       "  0.8312779068946838,\n",
       "  0.8339502215385437,\n",
       "  0.8353940844535828,\n",
       "  0.8368299007415771,\n",
       "  0.8374840617179871,\n",
       "  0.8382418751716614,\n",
       "  0.8393825888633728,\n",
       "  0.8394783139228821],\n",
       " 'val_loss': [0.3908167779445648,\n",
       "  0.36601167917251587,\n",
       "  0.3627864122390747,\n",
       "  0.3600771129131317,\n",
       "  0.3639492988586426,\n",
       "  0.3515869081020355,\n",
       "  0.354796826839447,\n",
       "  0.3541978895664215,\n",
       "  0.34959831833839417,\n",
       "  0.351121723651886],\n",
       " 'val_accuracy': [0.8255903124809265,\n",
       "  0.8305360674858093,\n",
       "  0.8327057957649231,\n",
       "  0.8333439826965332,\n",
       "  0.8306955695152283,\n",
       "  0.8346521854400635,\n",
       "  0.8334397077560425,\n",
       "  0.8346841335296631,\n",
       "  0.8347479104995728,\n",
       "  0.8353860974311829]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model.save(\"nn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
